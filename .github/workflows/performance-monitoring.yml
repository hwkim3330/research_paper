name: Continuous Performance Monitoring

on:
  schedule:
    - cron: '0 6 * * *'  # Daily at 6 AM UTC
  workflow_dispatch:  # Manual trigger
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - calculation
        - optimization  
        - scalability
        - memory
      iterations:
        description: 'Number of iterations'
        required: false
        default: '5'
        type: string
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '120'
        type: string

env:
  PYTHON_VERSION: '3.9'

jobs:
  performance-monitoring:
    name: Performance Monitoring
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run performance benchmarks
      run: |
        cd src
        python performance_benchmark.py \
          --test ${{ github.event.inputs.benchmark_type || 'all' }} \
          --iterations ${{ github.event.inputs.iterations || '5' }} \
          --duration ${{ github.event.inputs.duration || '120' }} \
          --output-dir ../performance_monitoring_results
    
    - name: Generate performance trends
      run: |
        cd src
        python -c "
import json
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Load benchmark results
results_file = '../performance_monitoring_results/benchmark_results.csv'
if Path(results_file).exists():
    df = pd.read_csv(results_file)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Create performance trends plot
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('CBS Performance Trends - $(date)', fontsize=16)
    
    # Plot 1: Success rate trends
    if not df.empty:
        axes[0,0].plot(df['timestamp'], df['success_rate'], 'o-')
        axes[0,0].set_title('Success Rate Over Time')
        axes[0,0].set_ylabel('Success Rate (%)')
        axes[0,0].grid(True, alpha=0.3)
        
        # Plot 2: Latency trends  
        axes[0,1].plot(df['timestamp'], df['avg_latency_ms'], 'o-', color='orange')
        axes[0,1].set_title('Average Latency Trends')
        axes[0,1].set_ylabel('Latency (ms)')
        axes[0,1].grid(True, alpha=0.3)
        axes[0,1].set_yscale('log')
        
        # Plot 3: Memory usage trends
        axes[1,0].plot(df['timestamp'], df['memory_usage_mb'], 'o-', color='green')
        axes[1,0].set_title('Memory Usage Trends')
        axes[1,0].set_ylabel('Memory (MB)')
        axes[1,0].grid(True, alpha=0.3)
        
        # Plot 4: CPU usage trends
        axes[1,1].plot(df['timestamp'], df['cpu_usage_percent'], 'o-', color='red')
        axes[1,1].set_title('CPU Usage Trends')
        axes[1,1].set_ylabel('CPU (%)')
        axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('../performance_monitoring_results/performance_trends.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print('Performance trends plot generated')
else:
    print('No benchmark results found')
"
        
    - name: Check for performance regressions
      run: |
        cd src
        python -c "
import pandas as pd
import json
from pathlib import Path

results_file = '../performance_monitoring_results/benchmark_results.csv'
regression_threshold = 0.2  # 20% performance degradation threshold

if Path(results_file).exists():
    df = pd.read_csv(results_file)
    
    # Simple regression detection (compare with hypothetical baseline)
    # In a real implementation, you would compare with historical data
    baseline_metrics = {
        'avg_latency_ms': 10.0,  # Example baseline
        'success_rate': 95.0,
        'cpu_usage_percent': 50.0,
        'memory_usage_mb': 500.0
    }
    
    regressions = []
    
    for _, row in df.iterrows():
        test_name = row['test_name']
        
        # Check latency regression
        if row['avg_latency_ms'] > baseline_metrics['avg_latency_ms'] * (1 + regression_threshold):
            regressions.append(f'{test_name}: Latency increased to {row[\"avg_latency_ms\"]:.2f}ms')
            
        # Check success rate regression  
        if row['success_rate'] < baseline_metrics['success_rate'] * (1 - regression_threshold):
            regressions.append(f'{test_name}: Success rate dropped to {row[\"success_rate\"]:.1f}%')
            
        # Check memory regression
        if row['memory_usage_mb'] > baseline_metrics['memory_usage_mb'] * (1 + regression_threshold):
            regressions.append(f'{test_name}: Memory usage increased to {row[\"memory_usage_mb\"]:.1f}MB')
    
    if regressions:
        print('‚ö†Ô∏è Performance regressions detected:')
        for regression in regressions:
            print(f'  - {regression}')
        
        # Save regression report
        with open('../performance_monitoring_results/regression_report.json', 'w') as f:
            json.dump({
                'timestamp': pd.Timestamp.now().isoformat(),
                'regressions': regressions,
                'threshold': regression_threshold
            }, f, indent=2)
            
        exit(1)  # Fail the job if regressions detected
    else:
        print('‚úÖ No performance regressions detected')
else:
    print('No benchmark results to analyze')
"
    
    - name: Upload performance monitoring results
      uses: actions/upload-artifact@v3
      with:
        name: performance-monitoring-${{ github.run_number }}
        path: performance_monitoring_results/
        retention-days: 90
        
    - name: Update performance history
      run: |
        # In a real implementation, you would append to a historical database
        # For this example, we just create a history entry
        mkdir -p performance_history
        cp performance_monitoring_results/benchmark_results.csv "performance_history/results_$(date +%Y%m%d_%H%M%S).csv"
        
    - name: Generate performance summary
      run: |
        cd src
        python -c "
import pandas as pd
import json
from datetime import datetime
from pathlib import Path

results_file = '../performance_monitoring_results/benchmark_results.csv'

if Path(results_file).exists():
    df = pd.read_csv(results_file)
    
    summary = {
        'timestamp': datetime.now().isoformat(),
        'total_tests': len(df),
        'avg_success_rate': df['success_rate'].mean(),
        'avg_latency_ms': df['avg_latency_ms'].mean(),
        'avg_cpu_percent': df['cpu_usage_percent'].mean(),
        'avg_memory_mb': df['memory_usage_mb'].mean(),
        'test_results': df.to_dict('records')
    }
    
    with open('../performance_monitoring_results/performance_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    print('Performance Summary:')
    print(f'  Tests run: {summary[\"total_tests\"]}')
    print(f'  Avg success rate: {summary[\"avg_success_rate\"]:.1f}%')
    print(f'  Avg latency: {summary[\"avg_latency_ms\"]:.3f}ms')
    print(f'  Avg CPU usage: {summary[\"avg_cpu_percent\"]:.1f}%')  
    print(f'  Avg memory usage: {summary[\"avg_memory_mb\"]:.1f}MB')
else:
    print('No results to summarize')
"

    - name: Comment on PR (if triggered by PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'performance_monitoring_results/performance_summary.json';
          
          if (fs.existsSync(path)) {
            const summary = JSON.parse(fs.readFileSync(path, 'utf8'));
            
            const comment = `## üìä Performance Monitoring Results
            
**Tests Run:** ${summary.total_tests}
**Average Success Rate:** ${summary.avg_success_rate.toFixed(1)}%
**Average Latency:** ${summary.avg_latency_ms.toFixed(3)}ms
**Average CPU Usage:** ${summary.avg_cpu_percent.toFixed(1)}%
**Average Memory Usage:** ${summary.avg_memory_mb.toFixed(1)}MB

Full results are available in the workflow artifacts.`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

    - name: Notify on performance issues
      if: failure()
      run: |
        echo "üö® Performance monitoring detected issues!"
        echo "Check the regression report in artifacts for details."
        echo "This may indicate performance degradation that needs attention."